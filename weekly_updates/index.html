<!doctype html>
<html lang="en">
<head>
  <!-- in <head> or before </body> -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Weekly Updates (Pure HTML)</title>
  <style>
    :root {
  --bg: #f9fafb;        /* soft near-white background */
  --panel: #e6edf3;     /* light pastel blue/grey panels */
  --text: #1a1c1e;      /* dark but not harsh black text */
  --muted: #6b7280;     /* muted grey for secondary text */
  --accent: #3b82f6;    /* gentle blue accent */
  --ring: rgba(59, 130, 246, 0.3); /* subtle highlight for focus rings */
}

    html, body { height: 100%; }
    body {
      margin: 0; font-family: system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, sans-serif;
      background: var(--bg); color: var(--text);
    }
    .wrap { max-width: 1200px; margin: 0 auto; padding: 24px; }
    .title { font-size: clamp(28px, 3vw, 40px); margin: 0 0 8px; }
    .sub { color: var(--muted); margin: 0 0 24px; }

    .grid { display: grid; grid-template-columns: 300px 1fr; gap: 24px; }
    @media (max-width: 900px) { .grid { grid-template-columns: 1fr; } }

    .panel { background: var(--panel); border-radius: 16px; box-shadow: 0 6px 24px rgba(0,0,0,.35); }
    .sidebar { padding: 8px; }
    .list { list-style: none; margin: 0; padding: 8px; max-height: 70vh; overflow: auto; }
    .btn {
      display: block; width: 100%; text-align: left; padding: 12px 14px; margin: 6px 0;
      color: var(--text); background: transparent; border: 1px solid transparent;
      border-radius: 12px; cursor: pointer; font: inherit;
    }
    .btn:hover { background: rgba(255,255,255,.04); }
    .btn[aria-current="true"] { background: rgba(79,163,255,.08); border-color: var(--ring); }

    .content { padding: 20px 24px; }
    .content h2 { margin-top: 0; }
    .muted { color: var(--muted); }

    table { width: 100%; border-collapse: collapse; }
    th, td { padding: 8px 10px; border-bottom: 1px solid rgba(255,255,255,.08); text-align: left; }
    code { background: rgba(255,255,255,.08); padding: 2px 6px; border-radius: 6px; }
    a { color: var(--accent); }
  </style>
</head>
<body>
  <main class="wrap">
    <h1 class="title">Weekly Updates</h1>
    <!-- <p class="sub">Click a week on the left to view its content. Tip: You can deep‑link to a week with a hash like <code>#week-1</code>.</p> -->

    <div class="grid">
      <!-- Sidebar: Week list (auto-generated from sections below) -->
      <aside class="panel sidebar">
        <ul class="list" id="week-list" role="tablist" aria-label="Weeks"></ul>
      </aside>

      <!-- Main: Week content -->
      <section class="panel content" id="content" role="region" aria-live="polite"></section>
    </div>

    <!--
      HOW TO ADD CONTENT (no frameworks needed)
      ----------------------------------------
      1) Copy a <section class="week" data-week="week-#"> block below.
      2) Change data-week (must be unique) and the <h2> title.
      3) Paste your text, lists, tables, images inside the block.
      4) Done. The sidebar and routing update automatically.

      You can link directly to a week with #week-1, #week-2, etc.
    -->

    <!-- Your week entries live here (they stay hidden; JS reads them) -->
    <template id="weeks-source">
      <section class="week" data-week="week-1">
        <h2>Week 1 — Sept 11</h2>


<h2>Projects</h2>
<ol>
  <li>
    Digging into our dataset for flare predictions (<strong>SWAN-SF</strong>) and supervised machine learning through 
    <strong>Random Forest</strong> and <strong>kNN classifier</strong>.
  </li>
  <li>
    Using <strong>CCMC score cards</strong> and <strong>ensembling techniques</strong> to build a more robust/accurate 
    combination of space weather simulations.
  </li>
</ol>
<hr />

<h2>Topic Selection</h2>
<p>Pick <strong>1 or 2</strong> from the following topics:</p>
<ul>
  <li>Thompson sampling</li>
  <li>Multi-armed bandit algorithm</li>
  <li>Monte Carlo search tree</li>
  <li>Random Forest</li>
  <li>kNN Classifier</li>
  <li>SWAN-SF dataset</li>
</ul>

<div>
  <h4>KNN</h4>
  <ul>
    <li>The K - nearest algorithm is a classification algorithm most commonly used for classification but can be used for regression as well.</li>
    <li>Based on human cognition.</li>
    <li>Lazy learner: as it does not make an actual model until and unless it has to predict a new data point.</li>
    <li>Instance based: There is no global model. The prediction is based on the comparison of the query points with the training points.</li>
    <li>Non-parametric: No assumption about the underlying distribution of data.</li>
  </ul>

  <h4>Probabilistic interpretation: P(Y=c∣X=x)</h4>
  <ul>
    <li>Estimate the posterior probability of a new data point belonging to a particular class.</li>
    <li>Quantify the uncertainty of that assignment.</li>
  </ul>

  <figure class="research-figure">
  <img src="formal defination.png" alt="KNN Classification Example" />
</figure>

  <h4>Algorithm Workflow:</h4>
  <ul>
    <li>Choose hyperparameters (k, distance metric)</li>
    <li>Compute distances d(x, xi).</li>
    <li>Select k nearest neighbors.</li>
    <li>Take the majority of the classes in Classification or take the average in the regression.</li>
  </ul>

  <h4>Hyperparameter K</h4>
  <ul>
    <li>Trade-off between Bias and Variance
      <ul>
        <li>If k is too small, it will be very sensitive to the outliers. High Variance, Overfitting</li>
        <li>If k is too large, it will include many examples from the other class group. High Bias, Underfitting</li>
      </ul>
    </li>
    <li>Use cross-validation, where we split the data into train-test and experiment with different values of K.</li>
    <li>Rule of thumb is k &lt; sqrt(n), where n is the number of training examples.</li>
    <li>K should also be an odd number.</li>
  </ul>

  <h4>Distance Metric</h4>
  <p>The accuracy highly depends on the choice of the distance metric. Example: Q is near to C if Euclidean is considered, and near to D if Cosine is considered.</p>
  <h2>Distance metrics in KNN</h2>
    <ul>
      <li><strong>Numerical:</strong> Euclidean, Manhattan</li>
      <li><strong>Categorical:</strong> Hamming</li>
      <li><strong>High-dimensional:</strong> Cosine, Jaccard</li>
      <li><strong>Time series:</strong> DTW (Dynamic Time Warping)</li>
    </ul>
  <figure class="research-figure">
  <img src="distance_metric.png" alt="KNN Classification Example" />
</figure>

<figure class="research-figure">
  <img src="distance.png" alt="KNN Classification Example" />
</figure>

  <h4>Few Considerations</h4>
  <ul>
    <li><h5>Curse of Dimensionality</h5>
      <p>To reduce this curse we use feature selection or dimensionality reduction. Distance loses meaning in high dimensions.</p>
      <h6>Feature Selection</h6>
      <ul>
        <li>If the dimension is too high, the distance between two points becomes less distinguishable.</li>
        <li><strong>Filter method:</strong> Compares each feature with the target. 
          <ul>
            <li>Information gain, Chi-square, Correlation-based feature selection</li>
          </ul>
        </li>
        <li><strong>Wrapper Method:</strong> Tests multiple combinations of features and selects the best.
          <ul>
            <li>Forward selection: Start with one feature, add features until no improvement.</li>
            <li>Backward selection: Start with all features and remove one at a time.</li>
          </ul>
        </li>
        <li><strong>Embedded Method:</strong> Feature selection is built into the algorithm (e.g., weighted KNN).</li>
      </ul>

      <h6>Dimensionality Reduction</h6>
      <ul>
        <li>Eliminate attributes that do not impact decision boundaries.</li>
        <li>Remove redundant attributes, reducing overfitting.</li>
      </ul>
    </li>

    <figure class="research-figure">
  <img src="dimension_reduction.png" alt="KNN Classification Example" />
</figure>

    <li><h5>Feature Scaling</h5>
      <ul>
        <li>Normalize the scale. Linearly scale each dimension to have mean 0 and variance 1.</li>
        <li>Use either Standard Scaler or Min-Max Scaler.</li>
      </ul>
    </li>

    <figure class="research-figure">
  <img src="minmax.png" alt="KNN Classification Example" />
</figure>

    <li><h5>Complexity</h5>
      <p>For brute force nearest neighbor search: O(n·d) for n samples and d dimensions. Optimized with KD-tree or Ball-tree.</p>
      <ul>
        <li><strong>KD tree:</strong> Divides the space into multidimensional regions. Best for low dimensions.</li>
        <li><strong>Ball tree:</strong> Groups data into hyperspheres. Best for high dimensions.</li>
      </ul>
    </li>

    <li><h5>Imbalanced Data</h5>
      <p>KNN struggles with imbalanced data since the majority class dominates. Solution: resampling.</p>
    </li>
  </ul>

  <h4>Decision Boundary</h4>
  <ul>
    <li>KNN does not explicitly compute decision boundaries, but they can be inferred.</li>
    <li>In 2D, this is shown via Voronoi diagrams:
      <ul>
        <li>Shows how input space is divided into classes.</li>
        <li>Each line segment is equidistant between two opposing points.</li>
        <li>Boundaries change depending on K.</li>
      </ul>
    </li>
  </ul>

  <h4>Weighted KNN</h4>
  <p>Assigns more weight to closer neighbors.</p>

  <figure class="research-figure">
  <img src="wKnn.png" alt="KNN Classification Example" />
</figure>

<figure class="research-figure">
  <img src="wkk2.png" alt="KNN Classification Example" />
</figure>

  <h4>Adaptive Neighborhood Selection (radius-based KNN)</h4>
  <p>Instead of k neighbors, take all points within a radius r. Considers density.</p>

  <h4>Advantages</h4>
  <ul>
    <li>Easy to implement and debug.</li>
    <li>Can be used on data that cannot be converted to feature vectors.</li>
  </ul>

  <h4>Disadvantages</h4>
  <ul>
    <li>Complexity issues in high dimensions.</li>
    <li>Sensitive to noise.</li>
  </ul>
</div>

<div>
  <h4>Random Forest</h4>
  <p>
    This is an ensemble method of decision trees where multiple trees vote for the most popular outcome. 
    Though decision trees have their own advantages (scale irrelevant, robust to irrelevant features, very interpretable), 
    they are prone to overfitting. As depth increases, bias decreases while variance increases.
  </p>
  <p>
    To resolve this issue, the Random Forest classifier comes into play. 
    Many decision trees are constructed, and the average or majority vote is considered.
  </p>

  <h5>Decision Tree</h5>
  <ul>
    <li>Starts with the root node.</li>
    <li>Splits nodes into sub-branches until leaf nodes are reached.</li>
    <li><strong>Splitting Criteria:</strong>
      <ul>
        <li><strong>Entropy:</strong> Measures randomness. Lower entropy is better.</li>
        <li><strong>Gini Index:</strong> Probability of misclassification. Lower is better, and preferred due to computational efficiency.</li>
      </ul>
    </li>
    <li><strong>Information Gain:</strong> Amount of information extracted from a decision tree. Higher is better.</li>
  </ul>

  <figure class="research-figure">
  <img src="rf_3.png" alt="KNN Classification Example" />
</figure>

  <h5>Bootstrapping Aggregation (Bagging)</h5>
  <ul>
    <li>Trains multiple base models independently and in parallel.</li>
    <li>Subsets of training data are created by random sampling with replacement.</li>
    <li>Ensemble training on slightly different data reduces variance and improves accuracy.</li>
  </ul>

  <figure class="research-figure">
  <img src="bagging.png" alt="KNN Classification Example" />
</figure>

  <h5>Python Implementation Example</h5>
  <pre>
from sklearn.ensemble import BaggingClassifier 
from sklearn.tree import DecisionTreeClassifier 

bag_clf = BaggingClassifier(
    DecisionTreeClassifier(), 
    n_estimators=500, 
    max_samples=100, 
    bootstrap=True, 
    n_jobs=-1
) 
bag_clf.fit(X_train, y_train) 
y_pred = bag_clf.predict(X_test)
  </pre>
  <p>
    <strong>n_estimators</strong>: Number of trees. <br>
    <strong>max_samples</strong>: Number of samples per tree. <br>
    <strong>bootstrap=True</strong>: Enables sampling with replacement. <br>
    <strong>n_jobs=-1</strong>: Utilizes all available CPUs. 
  </p>

  <h5>Feature Importance</h5>
  <ul>
    <li><strong>Mean Decrease Gini (classification):</strong> Average decrease in Gini impurity when a feature is used for splitting.
      <ul>
        <li>Calculate Gini index across all trees.</li>
        <li>Sum decreases for a feature across all trees.</li>
        <li>Divide by total number of trees to find the average.</li>
      </ul>
    </li>
    <li><strong>Mean Decrease Accuracy (regression):</strong> Average decrease in accuracy when a feature is permuted or removed.</li>
  </ul>

  <figure class="research-figure">
  <img src="gini.png" alt="KNN Classification Example" />
</figure>

  <h5>Out-of-Bag (OOB) Score</h5>
  <ul>
    <li>Samples not used in training a given tree are "out of bag".</li>
    <li>OOB score = number of correctly predicted OOB rows.</li>
    <li>Helps prevent data leakage, yielding lower variance and better generalization.</li>
  </ul>

  <h5>Hyperparameters</h5>
  <ul>
    <li><strong>Number of Trees (B):</strong> More trees → better predictions, but higher computational cost.</li>
    <li><strong>Features per Split (mtry):</strong>
      <ul>
        <li>Small mtry → less correlated trees, high bias, low variance.</li>
        <li>Large mtry → stronger splits, low bias, high variance.</li>
      </ul>
    </li>
  </ul>

  <h5>Computational Aspect</h5>
  <p>Time complexity: O(B * n log n), where n = number of training samples.</p>

  <h5>Advantages</h5>
  <ul>
    <li>Versatile: Performs well even without fine-tuned hyperparameters.</li>
    <li>Reduces overfitting by aggregating multiple decision trees.</li>
  </ul>

  <h5>Disadvantages</h5>
  <ul>
    <li>Computationally expensive: Large forests require significant training time. 
        Mitigated by parallel training.</li>
    <li>Reduced interpretability compared to a single decision tree.</li>
  </ul>

 
    <h1>Considerations during implementation</h1>

    <h2>Class weight — Random Forest</h2>
    <ul>
      <li>If classes are imbalanced, assign weights to classes.</li>
      <li>Common options:
        <ul class="sub">
          <li><strong>balanced</strong>: <span class="code">n_samples / (n_classes * np.bincount(y))</span></li>
          <li><strong>balanced_subsample</strong>: uses only the bootstrap (bag) sample for each tree</li>
          <li><strong>dictionary</strong>: create a dict mapping class → weight and pass it</li>
        </ul>
      </li>
      <li><strong>SMOTE</strong> (Synthetic Minority Oversampling Technique) — often uses KNN:
        <ul class="sub">
          <li>Generates synthetic minority-class samples.</li>
          <li>Synthesizes new points along the Euclidean line between minority neighbors.</li>
        </ul>
      </li>
    </ul>

    <h2>Evaluation metrics</h2>
    <ul>
      <li>Recall, Precision, F1-score</li>
      <li>ROC — AUC</li>
      <li>HSS (Heidke Skill Score) and TSS (True Skill Statistic)</li>
    </ul>


    <p>(How much I am better at finding real events)</p>
<p>$$
TSS = \frac{TP}{TP + FN} - \frac{FP}{FP + TN}
$$</p>

<p>(Forecast is better than random one)</p>
<p>$$
HSS = \frac{2 \times (TP \times TN - FN \times FP)}
{(P \times (FN + TN) + (TP + FP) \times N)}
$$</p>


  



  <a href="https://colab.research.google.com/drive/1xA4syEVXdSapE-MNAPmKR25Wq9oXJ11Y#scrollTo=UBd5I9-RiEEd" target="_blank" rel="noopener noreferrer">Collab</a>

</div>



      </section>

      <section class="week" data-week="week-2">
        <h2>Week 2 — Sept 25</h2>
        <!-- ✅ Paste this inside your <body> -->
<div class="section">
  <h3>Minkowski Distance</h3>
  <p>Generalization of both Euclidean and Manhattan distances.</p>

  <p>$$
  D(x,y) = \left( \sum_{i=1}^n |x_i - y_i|^p \right)^{1/p}
  $$</p>

  <p>When \( p = 1 \), it becomes the <b>Manhattan</b> or <b>L1 norm</b>.
  It measures the distance between two points along axes at right angles.</p>

  <p>The two points are \( p=(p_1,p_2,p_3,\ldots,p_n) \) and \( q=(q_1,q_2,\ldots,q_n) \):</p>

  <p>$$
  d_T(p,q) = \|p - q\|_T = \sum_{i=1}^n |p_i - q_i|
  $$</p>

  <p>The distance becomes less meaningful as dimensionality increases, so feature scaling matters a lot.</p>

  <h4>Applications</h4>
  <ul>
    <li>Path finding and Geographical Information Systems (GIS)</li>
    <li>High-dimensional data (e.g., image processing, text analysis)</li>
    <li>Clustering algorithms (reduce outlier impact)</li>
    <li>Anomaly detection (identify outliers robustly)</li>
  </ul>

  <p>The set of points with the same Manhattan length \( \|x\|_1 = c \)
  forms a diamond (square tilted \( 45^\circ \)), hence Manhattan distance is not rotation invariant.</p>

  <h3>Euclidean Distance</h3>
  <p><b>Properties:</b></p>
  <ul>
    <li>Symmetric for all points \( p, q \): \( d(p,q) = d(q,p) \)</li>
    <li>Always positive</li>
    <li>Triangle inequality: \( d(p,q) + d(q,r) \geq d(p,r) \)</li>
  </ul>

   <p>It is generally used in low dimensional continous data. </p>

  <p>If a point is rotated around the origin, its Euclidean length \( \|x\|_2 \) remains constant.
  Thus, Euclidean distance is rotation invariant.</p>

  <h3>Chebyshev Distance</h3>
  <ul>
    <li>Measures the greatest distance between any coordinate dimensions (also called “chessboard distance”).</li>
    <li>The Chebyshev distance between two vectors \( a \) and \( b \) is:</li>
  </ul>

  <p>$$
  D(a,b) = \max(|a_i - b_i|)
  $$</p>

  <p>This is used when the worst case matters more than the average i.e the tolerance checking </p>

  <p>$$
  D_{\text{chebyshev}}(x,y) = \lim_{p \to \infty}
  \left( \sum_i |x_i - y_i|^p \right)^{1/p} = \max_i |x_i - y_i|
  $$</p>

  <h4>Relations to Other Norms</h4>
  <p>For any \( x \in \mathbb{R}^d \):</p>

  <p>$$
  \|x\|_\infty \leq \|x\|_2 \leq \|x\|_1, \quad
  \|x\|_2 \leq \sqrt{d}\|x\|_\infty, \quad
  \|x\|_1 \leq d\|x\|_\infty
  $$</p>

  <p>The higher the value of \( p \) in Minkowski distance, the smaller the magnitude of distance becomes.
  The parameter \( p \) is usually tuned using cross-validation to optimize accuracy.</p>

  <p>As \( p \) increases, the distance metric becomes more sensitive to larger differences along any single dimension</p>
  <ul>
    <li>When \( p \)=1: all dimensions contribute linearly → robust to outliers.</li>
    <li>When \( p \)=2: alarge deviations dominate due to squaring.</li>
    <li>When \( p  \to \infty \): only the largest coordinate difference matters → Chebyshev distance.</li>
  </ul>

  <p>Minkowski distance is not rotation invariant except for Euclidean case.</p>
</div>


<div class="section">
  <h3>Kronecker Delta</h3>
  <p>The Kronecker delta is a function of two variables, commonly denoted as \( \delta_{ij} \).</p>

  <p>$$
  \delta_{ij} =
  \begin{cases}
    0 & \text{if } i \neq j, \\
    1 & \text{if } i = j.
  \end{cases}
  $$</p>

  <p>The value is equal to 1 when the indices are equal, and 0 otherwise. This is equivalent to the components of the identity matrix.
  For example, having row \( i \) and column \( j \), the element of the identity matrix is the same as the Kronecker delta:</p>

  <p>$$
  I_{ij} = \delta_{ij}
  $$</p>

  <h4>Properties of the Kronecker Delta</h4>

  <p>$$
  \sum_j \delta_{ij} a_j = a_i
  $$</p>

  <p>$$
  \sum_i a_i \delta_{ij} = a_j
  $$</p>

  <p>$$
  \sum_k \delta_{ik} \delta_{kj} = \delta_{ij}
  $$</p>
</div>

<h3>Curse of Dimensionality</h3>

  <p>
    It refers to the problem that arises due to having many dimensions or many features:
  </p>
  <ul>
    <li>Volume grows exponentially.</li>
    <li>Data becomes sparse, making distance lose its meaning.</li>
  </ul>

  <p>
    Hence all of the distance metrics lose their meaning. As all points start to look isolated, KNN tends to overfit.  
    Computation becomes \( O(n \cdot d) \).
  </p>

  <p>
    Let \( l \) be the edge of the smallest hypercube that contains all the k-nearest neighbours, then  
    the volume of the cube can be written as \( l^d \).  
    Also, the volume of the cube that has \( k \) points out of \( n \) total points can be written as \( \frac{k}{n} \).
  </p>

  <p>
    \[
    l \sim \left(\frac{k}{n}\right)^{1/d}
    \]
  </p>

  <p>If we consider a constant value of 1000 in 10, we can see that:</p>

<table style="
  border-collapse: collapse;
  width: 60%;
  margin: 20px 0;
  background: #ffffff;
  border: 2px solid #c2c9d6;
  box-shadow: 0 2px 5px rgba(0,0,0,0.1);
  font-family: Arial, sans-serif;
">
  <thead style="background-color: #e9eef6;">
    <tr>
      <th style="border: 1px solid #aab2bd; padding: 10px; text-align: center;">\( d \)</th>
      <th style="border: 1px solid #aab2bd; padding: 10px; text-align: center;">\( l\)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="border: 1px solid #aab2bd; padding: 10px; text-align: center;">2</td>
      <td style="border: 1px solid #aab2bd; padding: 10px; text-align: center;">0.1</td>
    </tr>
    <tr style="background-color: #f7f9fc;">
      <td style="border: 1px solid #aab2bd; padding: 10px; text-align: center;">10</td>
      <td style="border: 1px solid #aab2bd; padding: 10px; text-align: center;">0.63</td>
    </tr>
    <tr>
      <td style="border: 1px solid #aab2bd; padding: 10px; text-align: center;">100</td>
      <td style="border: 1px solid #aab2bd; padding: 10px; text-align: center;">0.955</td>
    </tr>
    <tr style="background-color: #f7f9fc;">
      <td style="border: 1px solid #aab2bd; padding: 10px; text-align: center;">1000</td>
      <td style="border: 1px solid #aab2bd; padding: 10px; text-align: center;">0.954</td>
    </tr>
  </tbody>
</table>


  <p>
    We can see that as the value of \( d \) (i.e., the dimension) increases, the length \( l \) of the small cube becomes nearly equal to that of the big cube.  
    This means the single nearest neighbour seems to cover the entire cube space.  
    That’s what is meant by *losing the meaning of distance*.
  </p>

  <h4>Mitigation</h4>

  <ul>
    <li><strong>Feature Selection</strong>
      <ul>
        <li>Remove irrelevant features and keep only the relevant ones, identified via domain knowledge or methods such as mutual information or variance thresholding.</li>
        <li><strong>Filter Method:</strong>
          <ul>
            <li>Compares each feature with the target; features with high correlation are selected.</li>
            <li>Examples: Information Gain, Chi-square, Correlation-based Feature Selection (Pearson for continuous, ANOVA for categorical).</li>
          </ul>
        </li>
        <li><strong>Wrapper Method:</strong>
          <ul>
            <li>Tests multiple feature combinations and chooses the best one.</li>
            <li>Forward Selection: Start with one feature, keep adding until performance stops improving.</li>
            <li>Backward Selection: Start with all features and remove one at a time.</li>
          </ul>
        </li>
        <li><strong>Embedded Method:</strong>
          <ul>
            <li>Feature selection is built into the algorithm — for example, weighted KNN.</li>
          </ul>
        </li>
      </ul>
    </li>

    <li><strong>Dimensionality Reduction</strong>
      <ul>
        <li>PCA (Principal Component Analysis)</li>
      </ul>
    </li>
  </ul>

  <h2>KD Tree</h2>
  <p>
    A KD Tree (K-Dimensional Tree) is a data structure that organizes data points in k-dimensional space.
    It recursively divides the space into half-spaces using hyperplanes that are orthogonal to one of the coordinate axes.
  </p>

  <h3>Algorithm</h3>
  <ul>
    <li>Choose a splitting dimension (usually cycles through axes, e.g., x, y, z, …).</li>
    <li>Find the median point along that dimension (to balance the tree).</li>
    <li>Create a node storing that point and split the dataset into:
      <ul>
        <li><strong>Left subtree</strong>: points with smaller coordinate values.</li>
        <li><strong>Right subtree</strong>: points with larger coordinate values.</li>
      </ul>
    </li>
    <li>Recurse on both subsets.</li>
  </ul>

  <p>
    The result is a balanced binary tree with construction time complexity of:
    \[
    O(n \log n)
    \]
  </p>

  <h3>Nearest Neighbor Search</h3>
  <ul>
    <li>Move through the binary tree until the nearest plane is found.</li>
    <li>Keep track of the current best distance \( r \) — the radius of the hypersphere around the query point \( q \).</li>
    <li>Check if the hypersphere intersects the splitting plane.</li>
    <li>If it does, recursively search the other branch — a closer neighbor might exist across the boundary.</li>
  </ul>

  <h3>Adding a Node</h3>
  <p>
    Similar to a Binary Search Tree insertion — find the correct leaf position based on the splitting dimension.
  </p>

  <h3>Deleting a Node</h3>
  <ul>
    <li>If the node to be deleted has children on the right, find the node with the <strong>minimum value</strong> along the target axis from the right subtree.</li>
    <li>Otherwise, find the point with the <strong>maximum value</strong> from the subtree rooted at the left child.</li>
  </ul>

  <p>
    Performance deteriorates rapidly with dimensionality when \( d > 20 \), due to the <em>curse of dimensionality</em> — most points become almost equidistant, and the tree loses pruning power.
  </p>

  <h2>ANNOY (Approximate Nearest Neighbors Oh Yeah)</h2>
  <p>
    ANNOY is a data structure designed to find approximate nearest neighbors efficiently.  
    In high-dimensional spaces, an exact search (like KD Trees) becomes inefficient — typically \( O(n) \), where \( n \) is the number of neighbors.
    ANNOY reduces this to approximately \( O(\log n) \).
  </p>

  <h3>Algorithm</h3>
  <ul>
    <li>Pick two random points and split them at their midpoint along a plane perpendicular to the line joining them.</li>
    <li>Based on this split, create a binary tree where each node represents one section of the space.</li>
    <li>Repeat the process until the <em>K</em> nearest points lie within a single leaf node.</li>
  </ul>

  <h3>Issues</h3>
  <ul>
    <li>Some nearest neighbors may be missed if they lie outside the current search region.</li>
    <li>It is not possible to find more than <em>K</em> neighbors using a single tree traversal.</li>
  </ul>

  <h3>Mitigation</h3>
  <ul>
    <li>While traversing, when the search is near enough, consider multiple splits instead of one.</li>
    <li>Create a <strong>forest of random trees</strong> — searching across multiple trees improves accuracy.</li>
  </ul>

  <p>
    This approach provides a trade-off between accuracy and speed, where the nearest neighbors found are approximate, not exact.
  </p>


  </section>

      <section class="week" data-week="week-3">
        <h2>Week 3 — OCT 23</h2>
       
<div class="section">
  <body>

  <h2>Experiment Setup and Methodology</h2>

  <p>
    In this experiment, we generated 1,000 random data points and evaluated the performance of nearest neighbor search algorithms across varying dimensionalities.
    The dimensional range used was:
  </p>

  <p>\[
  D_s = [16, 64, 256, 1024, 4096]
  \]</p>
  <p>Similarly we kept the dimension constant and noted the build and the query time for the kd and annoy algorithm. Here the dimension was kept contant at 128</p>
  <p>\[n_s = [1000,2000,3000,4000]\]</p>

  <p>
    These dimensions were selected to study algorithm performance under high-dimensional settings.
    We conducted nearest neighbor searches for 50,000 query points to obtain a stable average and suppress random noise in measurements.
  </p>

  <h3>Algorithms Compared</h3>
  <ol>
    <li><b>Brute Force Search</b> – served as the ground truth for accuracy evaluation.</li>
    <li><b>KD-Tree Search</b> – a space-partitioning structure for efficient nearest neighbor queries in moderate dimensions.</li>
    <li><b>Annoy Tree (Approximate Nearest Neighbors Oh Yeah)</b> – designed for high-dimensional data and large-scale approximate search.</li>
  </ol>

  <p>
    For each algorithm, we recorded the indices of the nearest neighbors found. 
    Additionally, for the KD-Tree and Annoy Tree methods, we measured:
  </p>
  <ul>
    <li><b>Build Time</b>: Time required to construct the tree structure.</li>
    <li><b>Query Time</b>: Time taken to retrieve the nearest neighbors for all query points.</li>
  </ul>

  While running the experiment we found the following data 
  <h3>Figure: Query and Build Time</h3>
  <div class="figure-placeholder"> <img src="plot.png" alt="plot" /></div>
  <div class="figure-placeholder"> <img src="plot_n.png" alt="plot" /></div>


  <h3>Results and Discussion</h3>

  <h3>1. Query Time and Build Time Analysis</h3>
  <p>
    The KD-Tree query time increases rapidly with higher dimensionality, showing that its performance deteriorates as the number of dimensions grows.
    The relationship between query time and dimensionality is approximately linear.
  </p>

    <h3>Figure: Time Comparison</h3>
 
  <div class="figure-placeholder"> <img src="time_comparison.png" alt="time Comparison example" width="800" height="400"/></div>

  <p>
    In contrast, the Annoy Tree demonstrates significantly better scalability.
    For instance, at a dimensionality of 4096, the KD-Tree required more than 400 seconds, whereas the Annoy Tree completed the same task in approximately 100 seconds.
  </p>
  <p>
    Quantitatively, Annoy was:
    <br>
    2.1×, 6.9×, 14.3×, 7.9×, and 4.3× faster than KD-Tree at dimensions 16, 64, 256, 1024, and 4096, respectively.
  </p>

  
  <h2>Query in terms of n </h2>
  <div class="figure-placeholder"> <img src="time_n.png" alt="plot" width="700" height = '400' /></div>
  <p>We can see that the effect of n is also similar to the D where the time for higher n is more in annoy tree as annoy has to ccalculate more number of trees for those n. Whereas the query time is complety flat for annoy whereas near linear in kd </p>

  <h3>2. Accuracy (Recall) Comparison</h3>

  <p>
    We evaluated the accuracy of KD-Tree and Annoy Tree against the brute-force search (the ground truth) using a recall-based metric.
    For each query, we compared whether the nearest neighbor index found by KD or Annoy matched the brute-force result — assigning 1 for a correct match and 0 otherwise.
    The mean of these values gives the recall score.
  </p>

  <table>
    <thead>
      <tr>
        <th>Dimension (D)</th>
        <th>KD Recall</th>
        <th>Annoy Recall</th>
      </tr>
    </thead>
    <tbody>
      <tr><td>16</td><td>1.00000</td><td>0.80802</td></tr>
      <tr><td>64</td><td>1.00000</td><td>0.33640</td></tr>
      <tr><td>256</td><td>1.00000</td><td>0.40548</td></tr>
      <tr><td>1024</td><td>1.00000</td><td>1.00000</td></tr>
      <tr><td>4096</td><td>1.00000</td><td>1.00000</td></tr>
    </tbody>
  </table>

  

  <p>
    The KD-Tree achieves a perfect recall of 1.0 across all dimensions, confirming it always finds the exact nearest neighbor — but at the cost of speed.
    The Annoy Tree, being an approximate nearest neighbor algorithm, trades some accuracy for performance.
  </p>

  <h3>3. Effect of <i>search_k</i> Parameter</h3>
  <p>
    For Annoy, accuracy can be improved by tuning the <code>search_k</code> parameter. 
    This parameter controls the number of nodes explored during a query — higher values of <code>search_k</code> cause the algorithm to search more candidate nodes, improving recall but also increasing query time.
  </p>
  <h3>Figure: Recall Plot</h3>
  <div class="figure-placeholder"> <img src="recall.png" alt="recall" /></div>

  <h3>4. Memory Size Comparison</h3>
  <table>
    <thead>
      <tr>
        <th>Dimension (D)</th>
        <th>Annoy File Size (KB)</th>
        <th>KD File Size (KB)</th>
      </tr>
    </thead>
    <tbody>
      <tr><td>16</td><td>663.28</td><td>142.38</td></tr>
      <tr><td>64</td><td>825.03</td><td>540.63</td></tr>
      <tr><td>256</td><td>1553.91</td><td>2133.63</td></tr>
      <tr><td>1024</td><td>4417.19</td><td>8505.63</td></tr>
      <tr><td>4096</td><td>17617.19</td><td>33993.63</td></tr>
    </tbody>
  </table>

  <p>
    For lower dimensions (\( D < 65 \)), the memory size for the Annoy Tree is larger, as it constructs multiple random trees.
    For higher dimensions (\( D > 65 \)), the KD-Tree requires significantly more memory because it must store metadata for each partition.
    At \( D = 4096 \), the KD-Tree memory usage is nearly twice that of Annoy.
  </p>

</body>
  </div>

<section>
  <h2>Empirical Time Complexity Analysis</h2>

  <p>
    We estimate how execution time (<em>t</em>) scales with dimensionality (<em>d</em>) by assuming a power-law:
    <code>t = c · d^α</code>, where <code>α</code> is the scaling exponent and <code>c</code> is a constant.
    Taking logarithms yields <code>log t = log c + α · log d</code>, so the slope on a log–log plot of
    <code>t</code> vs <code>d</code> gives the empirical value of <code>α</code>.
  </p>

  <p><strong>Method validity:</strong> The log–log slope (power-law fit) approach is a standard, correct way to report
  empirical “Big-O” style scaling when varying one factor (here, dimension) under otherwise fixed conditions.</p>

  <h3>Estimated Exponents (α)</h3>
  <table border="1" cellpadding="6" cellspacing="0">
    <thead>
      <tr>
        <th>Method</th>
        <th>Build-time Exponent (α)</th>
        <th>Query-time Exponent (α)</th>
        <th>Complexity (Build)</th>
        <th>Complexity (Query)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>KD-Tree</strong></td>
        <td style="text-align:center;">0.80</td>
        <td style="text-align:center;">1.04</td>
        <td><code>O(d^0.80)</code></td>
        <td><code>O(d^1.04)</code></td>
      </tr>
      <tr>
        <td><strong>Annoy Tree</strong></td>
        <td style="text-align:center;">0.38</td>
        <td style="text-align:center;">0.81</td>
        <td><code>O(d^0.38)</code></td>
        <td><code>O(d^0.81)</code></td>
      </tr>
    </tbody>
  </table>

  <h3>Interpretation</h3>
  <ul>
    <li>
      <strong>Build time:</strong> Both KD-Tree and Annoy exhibit <em>sub-linear</em> scaling with dimension.
      KD-Tree’s exponent (~0.80) is higher than Annoy’s (~0.38), indicating Annoy’s build scales more gently as
      <code>d</code> increases.
    </li>
    <li>
      <strong>KD-Tree query:</strong> The query-time exponent is ~1.04, i.e., <em>approximately linear</em> in
      <code>d</code>, reflecting the curse-of-dimensionality effects on traversal and distance checks.
    </li>
    <li>
      <strong>Annoy query:</strong> The exponent is ~0.81 (sub-linear), indicating slower growth with dimension
      compared to KD-Tree under the tested settings.
    </li>
  </ul>
</section>


<table style="
  border-collapse: collapse;
  width: 100%;
  font-family: Arial, sans-serif;
  font-size: 13px;
  text-align: center;
  margin: 20px auto;
  table-layout: fixed;
  word-wrap: break-word;
">
  <thead>
    <tr style="background-color: #f2f2f2;">
      <th style="border: 1px solid #444; padding: 4px;">D</th>
      <th style="border: 1px solid #444; padding: 4px;">KD_build<br>(s)</th>
      <th style="border: 1px solid #444; padding: 4px;">KD_query<br>(s)</th>
      <th style="border: 1px solid #444; padding: 4px;">Annoy_build<br>(s)</th>
      <th style="border: 1px solid #444; padding: 4px;">Annoy_query<br>(s)</th>
      <th style="border: 1px solid #444; padding: 4px;">KD_build<br>α</th>
      <th style="border: 1px solid #444; padding: 4px;">KD_query<br>α</th>
      <th style="border: 1px solid #444; padding: 4px;">Annoy_build<br>α</th>
      <th style="border: 1px solid #444; padding: 4px;">Annoy_query<br>α</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="border: 1px solid #444; padding: 4px;">16</td>
      <td style="border: 1px solid #444; padding: 4px;">0.004263</td>
      <td style="border: 1px solid #444; padding: 4px;">1.35271</td>
      <td style="border: 1px solid #444; padding: 4px;">0.032974</td>
      <td style="border: 1px solid #444; padding: 4px;">1.114981</td>
      <td colspan="4" style="border: 1px solid #444; color: gray;">—</td>
    </tr>
    <tr style="background-color: #fafafa;">
      <td style="border: 1px solid #444; padding: 4px;">64</td>
      <td style="border: 1px solid #444; padding: 4px;">0.004386</td>
      <td style="border: 1px solid #444; padding: 4px;">5.520358</td>
      <td style="border: 1px solid #444; padding: 4px;">0.018827</td>
      <td style="border: 1px solid #444; padding: 4px;">0.93985</td>
      <td style="border: 1px solid #444; padding: 4px;">0.020483</td>
      <td style="border: 1px solid #444; padding: 4px;">1.014454</td>
      <td style="border: 1px solid #444; padding: 4px;">-0.40425</td>
      <td style="border: 1px solid #444; padding: 4px;">-0.123258</td>
    </tr>
    <tr>
      <td style="border: 1px solid #444; padding: 4px;">256</td>
      <td style="border: 1px solid #444; padding: 4px;">0.033696</td>
      <td style="border: 1px solid #444; padding: 4px;">26.368551</td>
      <td style="border: 1px solid #444; padding: 4px;">0.022434</td>
      <td style="border: 1px solid #444; padding: 4px;">1.988607</td>
      <td style="border: 1px solid #444; padding: 4px;">1.470819</td>
      <td style="border: 1px solid #444; padding: 4px;">1.127992</td>
      <td style="border: 1px solid #444; padding: 4px;">0.126448</td>
      <td style="border: 1px solid #444; padding: 4px;">0.540628</td>
    </tr>
    <tr style="background-color: #fafafa;">
      <td style="border: 1px solid #444; padding: 4px;">1024</td>
      <td style="border: 1px solid #444; padding: 4px;">0.150219</td>
      <td style="border: 1px solid #444; padding: 4px;">111.24092</td>
      <td style="border: 1px solid #444; padding: 4px;">0.055555</td>
      <td style="border: 1px solid #444; padding: 4px;">16.213309</td>
      <td style="border: 1px solid #444; padding: 4px;">1.078208</td>
      <td style="border: 1px solid #444; padding: 4px;">1.038399</td>
      <td style="border: 1px solid #444; padding: 4px;">0.654099</td>
      <td style="border: 1px solid #444; padding: 4px;">1.513674</td>
    </tr>
    <tr>
      <td style="border: 1px solid #444; padding: 4px;">4096</td>
      <td style="border: 1px solid #444; padding: 4px;">0.377542</td>
      <td style="border: 1px solid #444; padding: 4px;">451.057634</td>
      <td style="border: 1px solid #444; padding: 4px;">0.274202</td>
      <td style="border: 1px solid #444; padding: 4px;">99.347379</td>
      <td style="border: 1px solid #444; padding: 4px;">0.664784</td>
      <td style="border: 1px solid #444; padding: 4px;">1.009812</td>
      <td style="border: 1px solid #444; padding: 4px;">1.151629</td>
      <td style="border: 1px solid #444; padding: 4px;">1.307652</td>
    </tr>
  </tbody>
</table>



      </section>






<!-- ✅ Include this once before </body> if not already added -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">


</script>


      </section>

      
    </template>
  </main>

  <script>
    // Grab week sections from the template, build the sidebar, and handle routing.
    const src = document.getElementById('weeks-source');
    const weeks = Array.from(src.content.querySelectorAll('.week')).map(el => ({
      id: el.getAttribute('data-week'),
      title: el.querySelector('h2')?.textContent?.trim() || el.getAttribute('data-week'),
      html: el.innerHTML.trim()
    })).sort((a, b) => {
      const na = Number(a.id.match(/\d+/)?.[0] || 0);
      const nb = Number(b.id.match(/\d+/)?.[0] || 0);
      return na - nb;
    });

    const list = document.getElementById('week-list');
    const content = document.getElementById('content');

    function renderList(activeId) {
      list.innerHTML = '';
      weeks.forEach(w => {
        const li = document.createElement('li');
        const btn = document.createElement('button');
        btn.className = 'btn';
        btn.type = 'button';
        btn.role = 'tab';
        btn.setAttribute('aria-selected', String(w.id === activeId));
        if (w.id === activeId) btn.setAttribute('aria-current', 'true');
        btn.textContent = w.title;
        btn.addEventListener('click', () => navigate(w.id));
        li.appendChild(btn);
        list.appendChild(li);
      });
    }

    function renderContent(activeId) {
      const w = weeks.find(x => x.id === activeId) || weeks[0];
      if (!w) { content.innerHTML = '<p class="muted">No weeks yet. Add a section in the template.</p>'; return; }
      content.innerHTML = `<h2>${w.title}</h2>${w.html}`;
    }

    function navigate(id) {
      if (!id) return;
      if (location.hash !== '#' + id) history.replaceState(null, '', '#' + id);
      renderList(id);
      renderContent(id);
    }

    // Initialize from hash or default to first week
    window.addEventListener('hashchange', () => navigate(location.hash.replace('#','')));
    navigate(location.hash.replace('#','') || (weeks[0] && weeks[0].id));
  </script>
</body>
</html>
